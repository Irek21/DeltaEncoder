{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader():\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.reference_features = self.random_pairs(features, labels)\n",
    "        self.labels = labels\n",
    "        \n",
    "    def random_pairs(self, X, labels):\n",
    "        Y = X.copy()\n",
    "        for l in range(labels.shape[1]):\n",
    "            inds = np.where(labels[:, l])[0]\n",
    "            inds_pairs = np.random.permutation(inds)\n",
    "            Y[inds, :] = X[inds_pairs, :]\n",
    "        return Y\n",
    "    \n",
    "    def batch_load(self, start, end):\n",
    "        if start == 0:\n",
    "            idx = np.r_[:self.features.shape[0]]\n",
    "            np.random.shuffle(idx)\n",
    "            self.features = self.features[idx]\n",
    "            self.reference_features = self.reference_features[idx]\n",
    "            self.labels = self.labels[idx]\n",
    "            \n",
    "        if end > self.features.shape[0]:\n",
    "            end = self.features.shape[0]\n",
    "            \n",
    "        return self.features[start:end], self.reference_features[start:end], self.labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.load('Data/DEData/features_train.npy').astype('float32')\n",
    "labels_train = np.load('Data/DEData/labels_train.npy').astype('float32')\n",
    "loader = BatchLoader(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaEncoder(nn.Module):\n",
    "    def __init__(self, input_size=2048, hidden_size=8192, neck_size=16):\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size * 2, hidden_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_size, neck_size),\n",
    "        )\n",
    "        \n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(input_size + neck_size, hidden_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_size, input_size),\n",
    "        )\n",
    "        dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        super(DeltaEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, X1, X2):\n",
    "        out = self.dropout(X1)\n",
    "        out = torch.cat((out, X2), dim=1)\n",
    "        out = self.encoder(out)\n",
    "        \n",
    "        out = torch.cat((X2, out), dim=1)\n",
    "        out = self.decoder(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = DeltaEncoder(2048, 512, 8).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-learning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = nn.L1Loss(reduction='none')\n",
    "MSE = nn.MSELoss(reduction='none')\n",
    "def weighted_MAE(predict, target):\n",
    "    batch_size = predict.shape[0]\n",
    "    feature_size = predict.shape[1]\n",
    "\n",
    "    substract_norm = MSE(predict, target)\n",
    "    L2_norms = torch.sum(substract_norm, dim=1) + 10e-7\n",
    "    weights = substract_norm / L2_norms.reshape((batch_size, 1)).expand((batch_size, feature_size))\n",
    "\n",
    "    substract = MAE(predict, target)\n",
    "    losses = torch.sum(substract * weights, dim=1)\n",
    "    loss = torch.mean(losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(G.parameters(), lr=10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 10.37740707397461\n",
      "Epoch 0 Loss 9.422536849975586\n",
      "Epoch 0 Loss 8.206025123596191\n",
      "Epoch 0 Loss 7.442513942718506\n",
      "Epoch 0 Loss 7.036445617675781\n",
      "Epoch 0 Loss 6.651961326599121\n",
      "Epoch 0 Loss 6.4385857582092285\n",
      "Epoch 0 Loss 6.247102737426758\n",
      "Epoch 0 Loss 6.1639604568481445\n",
      "Epoch 0 Loss 6.139676094055176\n",
      "Epoch 1 Loss 6.125914096832275\n",
      "Epoch 1 Loss 6.021148681640625\n",
      "Epoch 1 Loss 6.0706305503845215\n",
      "Epoch 1 Loss 5.966450214385986\n",
      "Epoch 1 Loss 5.8865275382995605\n",
      "Epoch 1 Loss 5.88031005859375\n",
      "Epoch 1 Loss 5.821693420410156\n",
      "Epoch 1 Loss 5.85353946685791\n",
      "Epoch 1 Loss 5.846227645874023\n",
      "Epoch 1 Loss 5.7451019287109375\n",
      "Epoch 2 Loss 5.807949542999268\n",
      "Epoch 2 Loss 5.7820024490356445\n",
      "Epoch 2 Loss 5.650452136993408\n",
      "Epoch 2 Loss 5.7722930908203125\n",
      "Epoch 2 Loss 5.804440498352051\n",
      "Epoch 2 Loss 5.7323408126831055\n",
      "Epoch 2 Loss 5.738101959228516\n",
      "Epoch 2 Loss 5.6783270835876465\n",
      "Epoch 2 Loss 5.6808061599731445\n",
      "Epoch 2 Loss 5.676595211029053\n",
      "Epoch 3 Loss 5.694836616516113\n",
      "Epoch 3 Loss 5.6591715812683105\n",
      "Epoch 3 Loss 5.69883918762207\n",
      "Epoch 3 Loss 5.6053972244262695\n",
      "Epoch 3 Loss 5.61496639251709\n",
      "Epoch 3 Loss 5.56415319442749\n",
      "Epoch 3 Loss 5.583009243011475\n",
      "Epoch 3 Loss 5.630671501159668\n",
      "Epoch 3 Loss 5.5216522216796875\n",
      "Epoch 3 Loss 5.559041976928711\n",
      "Epoch 4 Loss 5.555647373199463\n",
      "Epoch 4 Loss 5.488075256347656\n",
      "Epoch 4 Loss 5.491966247558594\n",
      "Epoch 4 Loss 5.503393173217773\n",
      "Epoch 4 Loss 5.563989162445068\n",
      "Epoch 4 Loss 5.462113380432129\n",
      "Epoch 4 Loss 5.492213249206543\n",
      "Epoch 4 Loss 5.449497699737549\n",
      "Epoch 4 Loss 5.390538215637207\n",
      "Epoch 4 Loss 5.393955230712891\n",
      "Epoch 5 Loss 5.4697184562683105\n",
      "Epoch 5 Loss 5.517693042755127\n",
      "Epoch 5 Loss 5.459756851196289\n",
      "Epoch 5 Loss 5.394853591918945\n",
      "Epoch 5 Loss 5.446107387542725\n",
      "Epoch 5 Loss 5.42240047454834\n",
      "Epoch 5 Loss 5.345059394836426\n",
      "Epoch 5 Loss 5.357396125793457\n",
      "Epoch 5 Loss 5.33004093170166\n",
      "Epoch 5 Loss 5.384715557098389\n",
      "Epoch 6 Loss 5.384556770324707\n",
      "Epoch 6 Loss 5.2892255783081055\n",
      "Epoch 6 Loss 5.294053077697754\n",
      "Epoch 6 Loss 5.303868770599365\n",
      "Epoch 6 Loss 5.354587078094482\n",
      "Epoch 6 Loss 5.371400833129883\n",
      "Epoch 6 Loss 5.302186012268066\n",
      "Epoch 6 Loss 5.266228675842285\n",
      "Epoch 6 Loss 5.289880752563477\n",
      "Epoch 6 Loss 5.245367527008057\n",
      "Epoch 7 Loss 5.265129089355469\n",
      "Epoch 7 Loss 5.234344482421875\n",
      "Epoch 7 Loss 5.300546646118164\n",
      "Epoch 7 Loss 5.208273887634277\n",
      "Epoch 7 Loss 5.345381736755371\n",
      "Epoch 7 Loss 5.379348278045654\n",
      "Epoch 7 Loss 5.300284385681152\n",
      "Epoch 7 Loss 5.221914291381836\n",
      "Epoch 7 Loss 5.246509552001953\n",
      "Epoch 7 Loss 5.24990177154541\n",
      "Epoch 8 Loss 5.227738380432129\n",
      "Epoch 8 Loss 5.251931190490723\n",
      "Epoch 8 Loss 5.233773231506348\n",
      "Epoch 8 Loss 5.225573539733887\n",
      "Epoch 8 Loss 5.210868835449219\n",
      "Epoch 8 Loss 5.241639137268066\n",
      "Epoch 8 Loss 5.210664749145508\n",
      "Epoch 8 Loss 5.217525482177734\n",
      "Epoch 8 Loss 5.2235565185546875\n",
      "Epoch 8 Loss 5.278066635131836\n",
      "Epoch 9 Loss 5.246896743774414\n",
      "Epoch 9 Loss 5.248054504394531\n",
      "Epoch 9 Loss 5.20905065536499\n",
      "Epoch 9 Loss 5.188894271850586\n",
      "Epoch 9 Loss 5.267582416534424\n",
      "Epoch 9 Loss 5.1926350593566895\n",
      "Epoch 9 Loss 5.185809135437012\n",
      "Epoch 9 Loss 5.177433013916016\n",
      "Epoch 9 Loss 5.164608478546143\n",
      "Epoch 9 Loss 5.171184539794922\n",
      "Epoch 10 Loss 5.20371150970459\n",
      "Epoch 10 Loss 5.225406646728516\n",
      "Epoch 10 Loss 5.158853530883789\n",
      "Epoch 10 Loss 5.166487216949463\n",
      "Epoch 10 Loss 5.176368236541748\n",
      "Epoch 10 Loss 5.187265872955322\n",
      "Epoch 10 Loss 5.10839319229126\n",
      "Epoch 10 Loss 5.113696575164795\n",
      "Epoch 10 Loss 5.136438369750977\n",
      "Epoch 10 Loss 5.181278705596924\n",
      "Epoch 11 Loss 5.177302360534668\n",
      "Epoch 11 Loss 5.1209025382995605\n",
      "Epoch 11 Loss 5.175360202789307\n",
      "Epoch 11 Loss 5.092486381530762\n",
      "Epoch 11 Loss 5.0973615646362305\n",
      "Epoch 11 Loss 5.179548263549805\n",
      "Epoch 11 Loss 5.154500961303711\n",
      "Epoch 11 Loss 5.070523738861084\n",
      "Epoch 11 Loss 5.110072135925293\n",
      "Epoch 11 Loss 5.15919828414917\n",
      "Epoch 12 Loss 5.062639236450195\n",
      "Epoch 12 Loss 5.065164566040039\n",
      "Epoch 12 Loss 5.138726711273193\n",
      "Epoch 12 Loss 5.101162910461426\n",
      "Epoch 12 Loss 5.135180950164795\n",
      "Epoch 12 Loss 5.095608234405518\n",
      "Epoch 12 Loss 5.084563732147217\n",
      "Epoch 12 Loss 5.045017242431641\n",
      "Epoch 12 Loss 5.072704315185547\n",
      "Epoch 12 Loss 5.065090656280518\n",
      "Epoch 13 Loss 4.998026371002197\n",
      "Epoch 13 Loss 5.099222183227539\n",
      "Epoch 13 Loss 5.068819046020508\n",
      "Epoch 13 Loss 5.088943004608154\n",
      "Epoch 13 Loss 5.051823139190674\n",
      "Epoch 13 Loss 5.022905349731445\n",
      "Epoch 13 Loss 5.140468597412109\n",
      "Epoch 13 Loss 5.1147871017456055\n",
      "Epoch 13 Loss 5.075135707855225\n",
      "Epoch 13 Loss 5.158670902252197\n",
      "Epoch 14 Loss 5.066407203674316\n",
      "Epoch 14 Loss 5.116558074951172\n",
      "Epoch 14 Loss 5.057724475860596\n",
      "Epoch 14 Loss 5.086560249328613\n",
      "Epoch 14 Loss 5.135335922241211\n",
      "Epoch 14 Loss 5.0385589599609375\n",
      "Epoch 14 Loss 5.034825325012207\n",
      "Epoch 14 Loss 5.116759300231934\n",
      "Epoch 14 Loss 5.0760345458984375\n",
      "Epoch 14 Loss 5.041836261749268\n",
      "Epoch 15 Loss 5.018924236297607\n",
      "Epoch 15 Loss 5.085052490234375\n",
      "Epoch 15 Loss 5.025014400482178\n",
      "Epoch 15 Loss 5.058535575866699\n",
      "Epoch 15 Loss 5.047543525695801\n",
      "Epoch 15 Loss 5.102854251861572\n",
      "Epoch 15 Loss 5.164355278015137\n",
      "Epoch 15 Loss 5.035756587982178\n",
      "Epoch 15 Loss 5.0326151847839355\n",
      "Epoch 15 Loss 5.046504020690918\n",
      "Epoch 16 Loss 5.075602054595947\n",
      "Epoch 16 Loss 5.090212821960449\n",
      "Epoch 16 Loss 5.02617073059082\n",
      "Epoch 16 Loss 5.024559020996094\n",
      "Epoch 16 Loss 4.969076156616211\n",
      "Epoch 16 Loss 5.02937126159668\n",
      "Epoch 16 Loss 5.003222942352295\n",
      "Epoch 16 Loss 5.131920337677002\n",
      "Epoch 16 Loss 5.11345100402832\n",
      "Epoch 16 Loss 5.0394392013549805\n",
      "Epoch 17 Loss 4.990548133850098\n",
      "Epoch 17 Loss 5.030722141265869\n",
      "Epoch 17 Loss 5.019077301025391\n",
      "Epoch 17 Loss 5.0550150871276855\n",
      "Epoch 17 Loss 5.0071635246276855\n",
      "Epoch 17 Loss 4.989091396331787\n",
      "Epoch 17 Loss 5.028562068939209\n",
      "Epoch 17 Loss 5.053933143615723\n",
      "Epoch 17 Loss 5.0381293296813965\n",
      "Epoch 17 Loss 5.0073394775390625\n",
      "Epoch 18 Loss 4.970209121704102\n",
      "Epoch 18 Loss 4.996695518493652\n",
      "Epoch 18 Loss 5.023202896118164\n",
      "Epoch 18 Loss 4.972594738006592\n",
      "Epoch 18 Loss 4.976613998413086\n",
      "Epoch 18 Loss 5.049252510070801\n",
      "Epoch 18 Loss 5.098349571228027\n",
      "Epoch 18 Loss 4.988278865814209\n",
      "Epoch 18 Loss 5.007712364196777\n",
      "Epoch 18 Loss 5.087549209594727\n",
      "Epoch 19 Loss 4.991793632507324\n",
      "Epoch 19 Loss 4.966529846191406\n",
      "Epoch 19 Loss 5.040604114532471\n",
      "Epoch 19 Loss 5.026219844818115\n",
      "Epoch 19 Loss 4.949432373046875\n",
      "Epoch 19 Loss 4.95596170425415\n",
      "Epoch 19 Loss 5.020163059234619\n",
      "Epoch 19 Loss 5.095110893249512\n",
      "Epoch 19 Loss 4.9634175300598145\n",
      "Epoch 19 Loss 4.96946907043457\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "train_size = 48000\n",
    "\n",
    "for epoch in range(20):\n",
    "    for i in range(train_size // batch_size):\n",
    "        features, reference_features, labels = loader.batch_load(i * batch_size, (i + 1) * batch_size)\n",
    "        features = torch.tensor(features, device=device, dtype=torch.float32, requires_grad=False)\n",
    "        reference_features = torch.tensor(reference_features, device=device, dtype=torch.float32, requires_grad=False)\n",
    "        predict = G(features, reference_features)\n",
    "        \n",
    "        loss = weighted_MAE(predict, features)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i % 10 == 0):\n",
    "            print('Epoch {} Loss {}'.format(epoch, loss.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation & storing new samples (5 new instances per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaEncoderGenerator(nn.Module):\n",
    "    def __init__(self, input_size=2048, hidden_size=8192, neck_size=16):\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(input_size * 2, hidden_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_size, neck_size),\n",
    "        )\n",
    "        \n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(input_size + neck_size, hidden_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_size, input_size),\n",
    "        )\n",
    "        dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        super(DeltaEncoderGenerator, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, X1, X2, shot):\n",
    "        out = self.dropout(X1)\n",
    "        out = torch.cat((out, X2), dim=1)\n",
    "        out = self.encoder(out)\n",
    "        \n",
    "        out = torch.cat((shot, out), dim=1)\n",
    "        out = self.decoder(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_trained = DeltaEncoderGenerator(2048, 512, 8).to(device)\n",
    "G_trained.load_state_dict(G.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5, 2048])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_shots = 5\n",
    "episode = torch.zeros(1, 5, num_shots, 2048, device=device, requires_grad=False)\n",
    "\n",
    "for i in range(5):\n",
    "    with open('Data/PickledClasses/' + str(95 + i), 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "        \n",
    "    shot_numbers = np.random.randint(0, 600, size=1)\n",
    "    episode[0][i][:num_shots] = data['features'][shot_numbers]\n",
    "    \n",
    "episode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 processed.\n",
      "Class 1 processed.\n",
      "Class 2 processed.\n",
      "Class 3 processed.\n",
      "Class 4 processed.\n"
     ]
    }
   ],
   "source": [
    "# store samples to cpu!\n",
    "\n",
    "batch_size = 128\n",
    "gen_size = 1024\n",
    "train_size = 48000\n",
    "class_data = {\n",
    "    'label': 0,\n",
    "    'features': torch.zeros(gen_size, 2048, device=device, dtype=torch.float32, requires_grad=False)\n",
    "}\n",
    "\n",
    "for class_num in range(5):\n",
    "    indices = np.random.randint(low=0, high=train_size // batch_size, size=gen_size // batch_size)\n",
    "    j = 0\n",
    "    for i in indices:\n",
    "        features, reference_features, labels = loader.batch_load(i * batch_size, (i + 1) * batch_size)\n",
    "        features = torch.tensor(features, device=device, dtype=torch.float32, requires_grad=False)\n",
    "        reference_features = torch.tensor(reference_features, device=device, dtype=torch.float32, requires_grad=False)\n",
    "        \n",
    "        # shot = episode[0][class_num].expand(batch_size, 2048)\n",
    "        idx = np.arange(batch_size) % num_shots\n",
    "        shot = torch.zeros(batch_size, 2048)\n",
    "        shot[:] = episode[0, class_num, batch_size % num_shots]\n",
    "        class_data['features'][j * batch_size:(j + 1) * batch_size] = G_trained(features, reference_features, shot.to(device)).detach()\n",
    "        j += 1\n",
    "        \n",
    "    class_data['label'] = class_num\n",
    "    with open('Data/SynthFeatures/' + str(class_num), 'wb') as f:\n",
    "        pkl.dump(class_data, f)\n",
    "    print('Class {} processed.'.format(class_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package processed.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "class_size = 1024\n",
    "train_size = 48000\n",
    "pack_features = np.zeros((5, 2, 1024, 2048), dtype=np.float32)\n",
    "\n",
    "total_indices = np.random.permutation(train_size // batch_size)\n",
    "for class_num in range(5):\n",
    "    indices = total_indices[class_num * (class_size // batch_size):(class_num + 1) * (class_size // batch_size)]\n",
    "    j = 0\n",
    "    for i in indices:\n",
    "        features, reference_features, labels = loader.batch_load(i * batch_size, (i + 1) * batch_size)\n",
    "        pack_features[class_num][0][j * batch_size:(j + 1) * batch_size] = features\n",
    "        pack_features[class_num][1][j * batch_size:(j + 1) * batch_size] = reference_features\n",
    "        j += 1\n",
    "        \n",
    "with open('Data/SynthMaterial/0', 'wb') as f:\n",
    "    pkl.dump({'features': pack_features}, f)\n",
    "print('Package processed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training target classyfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader():\n",
    "    def __init__(self, class_size, num_classes, first_class, batch_size, batches_in_buff, path):\n",
    "        self.class_size = class_size\n",
    "        self.num_classes = num_classes\n",
    "        self.first_class = first_class\n",
    "        self.batch_size = batch_size\n",
    "        self.batches_in_buff = batches_in_buff\n",
    "        self.path = path\n",
    "        \n",
    "        self.indices = np.random.permutation(num_classes * class_size)\n",
    "        self.buff_size = batches_in_buff * batch_size\n",
    "        self.buff = [{'label': 0, 'features': torch.zeros(2048, device=device)} for i in range(self.buff_size)]\n",
    "        self.buff_num = 0\n",
    "    \n",
    "    def buff_gen(self, buff_num):\n",
    "        buff_indices = self.indices[buff_num * self.buff_size:(buff_num + 1) * self.buff_size]\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            with open(self.path + str(self.first_class + i), 'rb') as f:\n",
    "                class_data = pkl.load(f)\n",
    "\n",
    "            class_indices = np.where(((buff_indices < (i + 1) * self.class_size) & (buff_indices >= i * self.class_size)))[0]\n",
    "            for j in class_indices:\n",
    "                self.buff[j] = {\n",
    "                    'label': class_data['label'],\n",
    "                    'features': class_data['features'][buff_indices[j] % self.class_size]\n",
    "                }\n",
    "    \n",
    "    def batch_load(self, i):\n",
    "        buff_i = i % self.batches_in_buff\n",
    "        if (buff_i == 0):\n",
    "            self.buff_gen(self.buff_num)\n",
    "            self.buff_num += 1\n",
    "            \n",
    "        return self.buff[buff_i * self.batch_size:(buff_i + 1) * self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        fc_layers = nn.Sequential(\n",
    "            nn.Linear(2048, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = fc_layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss = 1.4712327718734741\n",
      "Epoch 1 Loss = 1.2442599534988403\n",
      "Epoch 2 Loss = 1.1964941024780273\n",
      "Epoch 3 Loss = 1.1142165660858154\n",
      "Epoch 4 Loss = 1.0954577922821045\n",
      "Epoch 5 Loss = 1.0322712659835815\n",
      "Epoch 6 Loss = 1.024875283241272\n"
     ]
    }
   ],
   "source": [
    "class_size = 1024\n",
    "num_classes = 5\n",
    "first_class = 0\n",
    "train_size = class_size * num_classes\n",
    "batch_size = 128\n",
    "batches_in_buff = 4\n",
    "buff_size = batch_size * batches_in_buff\n",
    "\n",
    "for epoch in range(7):\n",
    "    loader = BatchLoader(class_size, num_classes, first_class, batch_size, batches_in_buff, 'Data/SynthFeatures/')\n",
    "    for i in range(train_size // batch_size):\n",
    "        batch_tuple = loader.batch_load(i)\n",
    "        images = torch.zeros(batch_size, 2048, device=device, requires_grad=False)\n",
    "        labels = torch.zeros(batch_size, device=device, requires_grad=False, dtype=int)\n",
    "        for k in range(batch_size):\n",
    "            images[k] = batch_tuple[k]['features']\n",
    "            labels[k] = batch_tuple[k]['label']\n",
    "        \n",
    "        predict = classifier(images)\n",
    "        loss = criterion(predict, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.detach()\n",
    "    \n",
    "    # if (epoch % 10 == 0):\n",
    "    print('Epoch {} Loss = {}'.format(epoch, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on FSL task = 69.19999694824219 %\n"
     ]
    }
   ],
   "source": [
    "class_size = 600\n",
    "num_classes = 5\n",
    "first_class = 95\n",
    "train_size = class_size * 5\n",
    "batch_size = 100\n",
    "batches_in_buff = 10\n",
    "buff_size = batch_size * batches_in_buff\n",
    "loader = BatchLoader(class_size, num_classes, first_class, batch_size, batches_in_buff, 'Data/PickledClasses/')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(train_size // batch_size):\n",
    "    batch_tuple = loader.batch_load(i)\n",
    "    images = torch.zeros(batch_size, 2048, device=device, requires_grad=False)\n",
    "    labels = torch.zeros(batch_size, device=device, requires_grad=False, dtype=int)\n",
    "    for k in range(batch_size):\n",
    "        images[k] = batch_tuple[k]['features']\n",
    "        labels[k] = batch_tuple[k]['label'] - 95 # don't forget about this\n",
    "        \n",
    "    predict = classifier(images)\n",
    "    _, predicted = torch.max(predict.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('Accuracy on FSL task = {} %'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaEncoderGenerator(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=8, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=2056, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(G.to('cpu').state_dict(), 'Models/G')\n",
    "G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classyfier(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=5, bias=True)\n",
       "    (1): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(classyfier.to('cpu').state_dict(), 'Models/classyfier')\n",
    "classyfier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classyfier.load_state_dict(torch.load(\"Models/classyfier\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.load_state_dict(torch.load(\"Models/G\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
